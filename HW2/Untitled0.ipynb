{
 "metadata": {
  "name": "",
  "signature": "sha256:1b78b76124c86651c5fabef63698a07c7499257eaae80d73d8e75acbb8cf178d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkConf, SparkContext\n",
      "from pyspark.sql import SQLContext\n",
      "sqlContext = SQLContext(sc)\n",
      "\n",
      "from pyspark.mllib.recommendation import ALS, Rating\n",
      "\n",
      "import itertools, gc, math\n",
      "\n",
      "from pyechonest import config, song\n",
      "config.ECHO_NEST_API_KEY=\"secret\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def userStrToIntMap(data):\n",
      "    user = data.map(lambda x:x[0]).distinct().zipWithUniqueId()\n",
      "    return user\n",
      "\n",
      "def songStrToIntMap(data):\n",
      "    song = data.map(lambda x:x[1]).distinct().zipWithUniqueId()\n",
      "    return song\n",
      "\n",
      "def converUserAndSongToInt(data, user, song):\n",
      "    data = data.map(lambda x: (x[0], (x[1], x[2]))).join(user)   # (user, ((song, listenCnt), userInt))\n",
      "    data = data.map(lambda x: (x[1][1], x[1][0][0], x[1][0][1]))\n",
      "    data = data.map(lambda x: (x[1], (x[0], x[2]))).join(song)\n",
      "    data = data.map(lambda x: (x[1][0][0], x[1][1], int(x[1][0][1])))\n",
      "    return data\n",
      "\n",
      "def userReverseMap(user):\n",
      "    userReverse = user.map(lambda x: (x[1], x[0]))\n",
      "    return userReverse\n",
      "\n",
      "def songReverseMap(song):\n",
      "    songReverse = song.map(lambda x: (x[1], x[0]))\n",
      "    return song\n",
      "\n",
      "def songStrToName(songStr):\n",
      "    try:\n",
      "        return song.Song(songStr)\n",
      "    except IndexError:    # Echo nest API error sometimes happens\n",
      "        return\n",
      "\n",
      "def songIdToStr(songId):\n",
      "    return songReverse.filter(lambda x: x[0] == songId).collect()[0][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def computeRMSE(model, data):\n",
      "    n = data.count()\n",
      "    dataToUserSong = data.map(lambda r: (r[0], r[1]))\n",
      "    predictions = model.predictAll(dataToUserSong).map(lambda r: ((r[0], r[1]), r[2]))\n",
      "    ratesAndPreds = test2.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
      "    print 'before', ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
      "    RMSE = math.sqrt(ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
      "    return RMSE\n",
      "\n",
      "def findBestModel(train, validation, ranks, lambdas, alphas, iteration):\n",
      "    bestModel = None\n",
      "    bestRMSE = 10000000\n",
      "    bestRank = None\n",
      "    bestLambda = None\n",
      "    bestAlpha = None\n",
      "    \n",
      "    for rank, lmbda, alpha in itertools.product(ranks, lambdas, alphas):\n",
      "        try:\n",
      "            model = ALS.trainImplicit(train, rank=rank, iterations=iteration, lambda_=lmbda, alpha=alpha)\n",
      "            RMSE = computeRMSE(model, validation)\n",
      "            print rank, lmbda, alpha, RMSE\n",
      "            if RMSE < bestRmse:\n",
      "                bestModel = model\n",
      "                bestRMSE = RMSE\n",
      "                bestRank = rank\n",
      "                bestLambda = lmbda\n",
      "                bestAlpha = alpha\n",
      "        except:\n",
      "            pass\n",
      "        gc.collect()\n",
      "    return bestModel, bestRMSE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = sc.textFile('train_triplets.txt')\n",
      "data = data.map(lambda x: x.strip().split())\n",
      "\n",
      "user = userStrToIntMap(data)\n",
      "song = songStrToIntMap(data)\n",
      "\n",
      "data = converUserAndSongToInt(data, user, song)\n",
      "train6, validation2,test2 = data.randomSplit([6,2,2])\n",
      "\n",
      "ranks = [5, 10]\n",
      "lambdas = [0.01, 0.1]\n",
      "alphas = [0.01, 0.1]\n",
      "iteration = 5\n",
      "\n",
      "rank=10\n",
      "#bestModel, bestRMSE = findBestModel(train6, validation2, ranks, lambdas, alphas, iteration)\n",
      "model = model = ALS.trainImplicit(train6, rank, iteration, alpha=0.01)\n",
      "RMSE = computeRMSE(model, validation)\n",
      "print RMSE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o112.trainImplicitALSModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 84 in stage 190.0 failed 1 times, most recent failure: Lost task 84.0 in stage 190.0 (TID 4225, localhost): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:345)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat org.xerial.snappy.SnappyOutputStream.dumpOutput(SnappyOutputStream.java:343)\n\tat org.xerial.snappy.SnappyOutputStream.flush(SnappyOutputStream.java:318)\n\tat org.apache.spark.io.SnappyOutputStreamWrapper.flush(CompressionCodec.scala:201)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822)\n\tat java.io.ObjectOutputStream.flush(ObjectOutputStream.java:718)\n\tat org.apache.spark.serializer.JavaSerializationStream.flush(JavaSerializer.scala:57)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndClose(DiskBlockObjectWriter.scala:130)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:155)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1100)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1093)\n\tat org.apache.spark.ml.recommendation.ALS$.computeYtY(ALS.scala:1228)\n\tat org.apache.spark.ml.recommendation.ALS$.org$apache$spark$ml$recommendation$ALS$$computeFactors(ALS.scala:1169)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$train$3.apply(ALS.scala:633)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$train$3.apply(ALS.scala:621)\n\tat scala.collection.immutable.Range.foreach(Range.scala:141)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:621)\n\tat org.apache.spark.mllib.recommendation.ALS.run(ALS.scala:239)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainImplicitALSModel(PythonMLLibAPI.scala:505)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:345)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat org.xerial.snappy.SnappyOutputStream.dumpOutput(SnappyOutputStream.java:343)\n\tat org.xerial.snappy.SnappyOutputStream.flush(SnappyOutputStream.java:318)\n\tat org.apache.spark.io.SnappyOutputStreamWrapper.flush(CompressionCodec.scala:201)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822)\n\tat java.io.ObjectOutputStream.flush(ObjectOutputStream.java:718)\n\tat org.apache.spark.serializer.JavaSerializationStream.flush(JavaSerializer.scala:57)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndClose(DiskBlockObjectWriter.scala:130)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:155)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-8fc5d0006995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#bestModel, bestRMSE = findBestModel(train6, validation2, ranks, lambdas, alphas, iteration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainImplicit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mRMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeRMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jk/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/recommendation.pyc\u001b[0m in \u001b[0;36mtrainImplicit\u001b[0;34m(cls, ratings, rank, iterations, lambda_, blocks, alpha, nonnegative, seed)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \"\"\"\n\u001b[1;32m    258\u001b[0m         model = callMLlibFunc(\"trainImplicitALSModel\", cls._prepare(ratings), rank,\n\u001b[0;32m--> 259\u001b[0;31m                               iterations, lambda_, blocks, alpha, nonnegative, seed)\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatrixFactorizationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jk/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/common.pyc\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jk/spark-1.6.0-bin-hadoop2.6/python/pyspark/mllib/common.pyc\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jk/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jk/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jk/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o112.trainImplicitALSModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 84 in stage 190.0 failed 1 times, most recent failure: Lost task 84.0 in stage 190.0 (TID 4225, localhost): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:345)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat org.xerial.snappy.SnappyOutputStream.dumpOutput(SnappyOutputStream.java:343)\n\tat org.xerial.snappy.SnappyOutputStream.flush(SnappyOutputStream.java:318)\n\tat org.apache.spark.io.SnappyOutputStreamWrapper.flush(CompressionCodec.scala:201)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822)\n\tat java.io.ObjectOutputStream.flush(ObjectOutputStream.java:718)\n\tat org.apache.spark.serializer.JavaSerializationStream.flush(JavaSerializer.scala:57)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndClose(DiskBlockObjectWriter.scala:130)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:155)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1100)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1093)\n\tat org.apache.spark.ml.recommendation.ALS$.computeYtY(ALS.scala:1228)\n\tat org.apache.spark.ml.recommendation.ALS$.org$apache$spark$ml$recommendation$ALS$$computeFactors(ALS.scala:1169)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$train$3.apply(ALS.scala:633)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$train$3.apply(ALS.scala:621)\n\tat scala.collection.immutable.Range.foreach(Range.scala:141)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:621)\n\tat org.apache.spark.mllib.recommendation.ALS.run(ALS.scala:239)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainImplicitALSModel(PythonMLLibAPI.scala:505)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:345)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat org.xerial.snappy.SnappyOutputStream.dumpOutput(SnappyOutputStream.java:343)\n\tat org.xerial.snappy.SnappyOutputStream.flush(SnappyOutputStream.java:318)\n\tat org.apache.spark.io.SnappyOutputStreamWrapper.flush(CompressionCodec.scala:201)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1822)\n\tat java.io.ObjectOutputStream.flush(ObjectOutputStream.java:718)\n\tat org.apache.spark.serializer.JavaSerializationStream.flush(JavaSerializer.scala:57)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndClose(DiskBlockObjectWriter.scala:130)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:155)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def recommendSongs(userStr, numRec):\n",
      "    userId = userReverse.filter(lambda x: x[0] == userStr).collect()[0][1]\n",
      "    listenedSongs = data.filter(lambda x:x[0] == userId).map(lambda x: x[1]).collect()[:20]\n",
      "    listenedSongs = map(songStrToName, listenedSongs)\n",
      "    print listenedSongs\n",
      "    results = []\n",
      "#     res = model.recommendProducts(uStr, numRec)\n",
      "#     for r in res:\n",
      "#         songId = r.product\n",
      "#         songStr = songIdToStr(songId)\n",
      "#         try:\n",
      "#             songName = songStrToName(songStr)\n",
      "#             results.append(songName)\n",
      "#         except IndexError:\n",
      "#             pass\n",
      "    return results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for u in user.take(1):\n",
      "    uStr = u[1]\n",
      "    res = recommendSongs(uStr, 10)\n",
      "    print res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[<song - Talk Show Host>, None, <song - Helium Hearts>, <song - Planet Claire>, None, <song - Teclo>, <song - I'm So Bad (10\" EP)>, <song - Strange (2006 Digital Remaster)>, None, <song - Two Drummers Disappear>, <song - S. Soup>, <song - Beatrix (BBC Session - John Peel, 5th September 1984)>, <song - Speeeder>, <song - Build High>, <song - Ants>, <song - Private Idaho>, <song - New York City>, <song - Let Yourself Go>, <song - Best Imitation Of Myself>, None]\n",
        "[]\n"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}